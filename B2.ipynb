{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644fcddd-885e-4d59-a106-b8b607d9e79a",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69496649-7741-4580-8851-ff8113546ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gradient Boosting Regression is a machine learning technique for regression tasks that builds an ensemble of decision trees in a\n",
    "sequential manner. Each new tree is trained to correct the errors made by the previous trees, using the gradients of the loss function to \n",
    "make adjustments. This process of adding trees continues until a specified number of trees is reached or the model performance converges.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ad595-f59b-43e1-85ec-5c10c5ce9e3b",
   "metadata": {},
   "source": [
    "# Ans : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaeefe2-e992-46cf-ad21-214057ff22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.squeeze() + 1 + np.random.randn(100) * 2\n",
    "\n",
    "# Gradient Boosting Regressor from scratch\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.mean(y)\n",
    "        self.models.append(y_pred)\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - y_pred\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.models.append(tree)\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.models[0]\n",
    "        for tree in self.models[1:]:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Train the model\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gbr.fit(X, y)\n",
    "y_pred = gbr.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fabe6b-13ab-4daf-b528-d3551d8cf5a0",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60144e-587a-4476-aa9a-bd8dc726922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "gbr = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452eb7f0-fc27-4081-b1b9-680aa3a67856",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa833f8c-6303-413d-84e7-13d7fa1608cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A weak learner in Gradient Boosting is a model that performs slightly better than random guessing. It usually has limited predictive\n",
    "power and high bias. Decision trees with a small depth are commonly used as weak learners in gradient boosting.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293945ba-addf-4c4a-aecd-cfc5ceff1cb1",
   "metadata": {},
   "source": [
    "# Ans : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e9904-2460-4a67-9d6f-8771e5297dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The intuition behind Gradient Boosting is to sequentially add models to an ensemble in such a way that each new model corrects the errors\n",
    "made by the previously combined models. By using gradient descent to minimize the loss function, the algorithm focuses on areas where the\n",
    "previous models performed poorly, thus improving the overall performance of the ensemble.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d23e63-de27-496c-bd87-a08da08d04c8",
   "metadata": {},
   "source": [
    "# Ans : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6be3b5-6d86-401a-9c6a-ce2a656c0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc2b13-cf33-41e9-9c97-b8ce0e95bb78",
   "metadata": {},
   "source": [
    "# Ans : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08091be0-d630-47b0-b13c-94721cd0c5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
